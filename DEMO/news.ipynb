{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a financial analyst tasked with providing insights into recent news articles related to the financial industry. Here are some recent news articles:\n",
      "\n",
      "   **News:** Tesla's numbers disappoint again ... and the crowd goes wild again\n",
      "\n",
      "   **News:** Google Sheets is getting faster and more effective, and I can't wait to ditch Excel for good\n",
      "\n",
      "   **News:** LinkedIn’s video push appears to be working in 2025\n",
      "\n",
      "   **News:** Hear from Microsoft Security experts at these top cybersecurity events in 2025\n",
      "\n",
      "   **News:** Apple Loop: iPhone 17 Design, Tiktok Still Missing, iPhone SE Release Dates\n",
      "\n",
      "   **News:** Attackers used a public ASP.NET machine to conduct ViewState code injection attacks\n",
      "\n",
      "   **News:** U.S. CISA adds Microsoft Outlook, Sophos XG Firewall, and other flaws to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** Cisco addressed two critical flaws in its Identity Services Engine (ISE)\n",
      "\n",
      "   **News:** U.S. CISA adds Linux kernel flaw to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** U.S. CISA adds Microsoft .NET Framework, Apache OFBiz, and Paessler PRTG Network Monitor flaws to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** U.S. CISA adds Apple products’ flaw to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** Law enforcement seized the domains of HeartSender cybercrime marketplaces\n",
      "\n",
      "   **News:** Web Skimmer found on at least 17 websites, including Casio UK\n",
      "\n",
      "   **News:** Texas is the first state to ban DeepSeek on government devices\n",
      "\n",
      "   **News:** AMD fixed a flaw that allowed to load malicious microcode\n",
      "\n",
      "   **News:** DeepSeek database exposed highly sensitive information\n",
      "\n",
      "   **News:** Contec CMS8000 patient monitors contain a hidden backdoor\n",
      "\n",
      "   **News:** Coyote Banking Trojan targets Brazilian users, stealing data from 70+ financial apps and websites\n",
      "\n",
      "   **News:** TeamViewer fixed a vulnerability in Windows client and host applications\n",
      "\n",
      "   **News:** Italy’s data protection authority Garante blocked the DeepSeek AI platform\n",
      "\n",
      "   **News:** Google fixed actively exploited kernel zero-day flaw\n",
      "\n",
      "   **News:** Elon Musk ’s DOGE team granted ‘full access’ to sensitive Treasury systems. What are the risks?\n",
      "\n",
      "   **News:** WhatsApp disrupted a hacking campaign targeting journalists with Paragon spyware\n",
      "\n",
      "   **News:** Online food ordering and delivery platform GrubHub discloses a data breach\n",
      "\n",
      "   **News:** Netgear urges users to upgrade two flaws impacting WiFi router models\n",
      "\n",
      "   **News:** Community Health Center data breach impacted over 1 million patients\n",
      "\n",
      "   **News:** A ransomware attack forced New York Blood Center to reschedule appointments\n",
      "\n",
      "   **News:** PHP package Voyager flaws expose to one-click RCE exploits\n",
      "\n",
      "   **News:** Italy’s Data Protection Authority Garante requested information from Deepseek\n",
      "\n",
      "   **News:** Broadcom fixed information disclosure flaws in VMware Aria Operations\n",
      "\n",
      "Please analyze these articles and provide insights into any potential impacts on the financial industry Sentiment on the provided company.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_prompt(news_df):\n",
    "    prompt = \"You are a financial analyst tasked with providing insights into recent news articles related to the financial industry. Here are some recent news articles:\\n\\n\"\n",
    "\n",
    "    for index, row in news_df.iterrows():\n",
    "        title = row['title']\n",
    "        prompt += f\"   **News:** {title}\\n\\n\"\n",
    "\n",
    "    prompt += \"Please analyze these articles and provide insights into any potential impacts on the financial industry Sentiment on the provided company.\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Build the prompt\n",
    "prompt = build_prompt(preprocessed_news_df)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching news for: Apple Inc. (AAPL)\n",
      "⚠ Error fetching from Yahoo Finance: 'provider'\n",
      "\n",
      "✅ Successfully saved 60 news articles to AAPL_news.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "# Replace with your NewsAPI Key\n",
    "NEWSAPI_KEY = \"8afd555b84b84ee9a83a720ef0de8397\"\n",
    "\n",
    "# Initialize NewsAPI Client\n",
    "newsapi = NewsApiClient(api_key=NEWSAPI_KEY)\n",
    "\n",
    "# Ask user for stock ticker\n",
    "stock_symbol = input(\"Enter Stock Ticker (e.g., AAPL, TSLA, MSFT): \").upper()\n",
    "\n",
    "# Fetch company information from Yahoo Finance\n",
    "try:\n",
    "    stock_info = yf.Ticker(stock_symbol).info\n",
    "    company_name = stock_info.get(\"shortName\", stock_symbol)  # Use stock name or ticker\n",
    "    print(f\"\\nFetching news for: {company_name} ({stock_symbol})\")\n",
    "except Exception as e:\n",
    "    print(\"⚠ Unable to fetch stock details. Using ticker as the company name.\")\n",
    "    company_name = stock_symbol\n",
    "\n",
    "# Function to fetch news from NewsAPI\n",
    "def fetch_newsapi_news(company_name, max_pages=3):\n",
    "    articles_list = []\n",
    "    try:\n",
    "        for page in range(1, max_pages + 1):\n",
    "            news_data = newsapi.get_everything(\n",
    "                q=company_name,\n",
    "                language=\"en\",\n",
    "                sort_by=\"publishedAt\",\n",
    "                page_size=20,\n",
    "                page=page  # Fetch multiple pages\n",
    "            )\n",
    "            if \"articles\" in news_data:\n",
    "                for article in news_data[\"articles\"]:\n",
    "                    articles_list.append({\n",
    "                        \"source\": article[\"source\"][\"name\"],\n",
    "                        \"title\": article[\"title\"],\n",
    "                        \"description\": article[\"description\"],\n",
    "                        \"url\": article[\"url\"],\n",
    "                        \"published_at\": article[\"publishedAt\"],\n",
    "                        \"source_type\": \"NewsAPI\"\n",
    "                    })\n",
    "            if len(news_data[\"articles\"]) < 20:  # Stop if fewer than 20 articles on a page\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error fetching from NewsAPI: {e}\")\n",
    "\n",
    "    return articles_list\n",
    "\n",
    "\n",
    "# Function to fetch stock news from Yahoo Finance\n",
    "def fetch_yahoo_news(stock_symbol):\n",
    "    articles_list = []\n",
    "    try:\n",
    "        stock = yf.Ticker(stock_symbol)\n",
    "        news_data = stock.news  # Fetch news\n",
    "\n",
    "        for article in news_data:\n",
    "            articles_list.append({\n",
    "                \"source\": article[\"provider\"],\n",
    "                \"title\": article[\"title\"],\n",
    "                \"description\": article.get(\"summary\", \"\"),\n",
    "                \"url\": article[\"link\"],\n",
    "                \"published_at\": pd.to_datetime(article[\"providerPublishTime\"], unit=\"s\"),\n",
    "                \"source_type\": \"Yahoo Finance\"\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error fetching from Yahoo Finance: {e}\")\n",
    "\n",
    "    return articles_list\n",
    "\n",
    "# Fetch news from both sources\n",
    "newsapi_news = fetch_newsapi_news(company_name)\n",
    "yahoo_news = fetch_yahoo_news(stock_symbol)\n",
    "\n",
    "# Combine all news articles\n",
    "all_news = newsapi_news + yahoo_news\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_news)\n",
    "\n",
    "# Save to CSV file\n",
    "csv_filename = f\"{stock_symbol}_news.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"\\n✅ Successfully saved {len(df)} news articles to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Ensure DataFrame exists and is not empty\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠ No news data available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     exit()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure DataFrame exists and is not empty\n",
    "if df.empty:\n",
    "    print(\"⚠ No news data available.\")\n",
    "    exit()\n",
    "\n",
    "# Convert news articles into structured paragraphs\n",
    "df[\"formatted_news\"] = df.apply(\n",
    "    lambda row: (\n",
    "        f\"News {row.name + 1}: {row['title']}.\\n\"\n",
    "        f\"Source: {row['source']} | Published: {row['published_at']}.\\n\"\n",
    "        f\"Description: {row['description']}\\n\"\n",
    "        f\"Read more: {row['url']}\\n\"\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Combine all news into a single text block\n",
    "final_text = \"\\n\\n\".join(df[\"formatted_news\"])\n",
    "\n",
    "# Save formatted text to a file\n",
    "text_filename = \"formatted_stock_news.txt\"\n",
    "with open(text_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_text)\n",
    "\n",
    "print(f\"\\n✅ Successfully saved structured news to {text_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks stored in FAISS successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "\n",
    "import os \n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# Set Google API Key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDxhPMlJLbGHMvBzXbgV8ldG9-JlReq1Hg\"\n",
    "\n",
    "# Load Google Generative AI embeddings model\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    model=\"models/embedding-001\"  # Correct model name\n",
    ")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",temperature=0.3, max_tokens=500)\n",
    "# llm= OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "\n",
    "# Load text file\n",
    "file_path = \"formatted_stock_news.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()  # Read entire content of the file\n",
    "\n",
    "# Convert each text file into a Document object\n",
    "documents = [Document(page_content=text_data)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "chunked_docs = text_splitter.split_documents(documents)\n",
    "db = FAISS.from_documents(chunked_docs, embedding_model)\n",
    "\n",
    "# Save FAISS index for future use\n",
    "db.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"Chunks stored in FAISS successfully!\")\n",
    "\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "system_prompt = (\"\"\"\"\n",
    "                 Act As an Financial News Summarizer Analyst . I have Provided you some key Financial News about stocks news  give answer according to it \n",
    "\n",
    "\"\"\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response appended to company_overview.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\" Summarize All News With Analysis Try to cover All the News dont miss out Any News Specially News With Numbers indicators  . it Should be detailed    \"\"\"\n",
    "\n",
    "response = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "# Extract the answer text\n",
    "response_text = response[\"answer\"]\n",
    "\n",
    "# Save the response to a text file (Append mode)\n",
    "with open(\"news_summarizer.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "\n",
    "    file.write(response_text + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"Response appended to company_overview.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model\n",
    "chain.invoke({\"question\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mike Moore</td>\n",
       "      <td>Google Sheets is getting faster and more effec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Benjamin Lim</td>\n",
       "      <td>Hear from Microsoft Security experts at these ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gabe Gurwin</td>\n",
       "      <td>Every Game Delayed Right Now (2025 Edition)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pierluigi Paganini</td>\n",
       "      <td>Attackers used a public ASP.NET machine to con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pierluigi Paganini</td>\n",
       "      <td>Russia’s intelligence recruits Ukrainians for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                              title\n",
       "0          Mike Moore  Google Sheets is getting faster and more effec...\n",
       "1        Benjamin Lim  Hear from Microsoft Security experts at these ...\n",
       "2         Gabe Gurwin        Every Game Delayed Right Now (2025 Edition)\n",
       "3  Pierluigi Paganini  Attackers used a public ASP.NET machine to con...\n",
       "4  Pierluigi Paganini  Russia’s intelligence recruits Ukrainians for ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from newsapi import NewsApiClient\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_news(query, from_date, to_date, language='en', sort_by='relevancy', page_size=30, api_key='8afd555b84b84ee9a83a720ef0de8397'):\n",
    "    # Initialize the NewsAPI client\n",
    "    newsapi = NewsApiClient(api_key=api_key)\n",
    "    query = query.replace(' ','&')\n",
    "    # Fetch all articles matching the query\n",
    "    all_articles = newsapi.get_everything(\n",
    "        q=query,\n",
    "        from_param=from_date,\n",
    "        to=to_date,\n",
    "        language=language,\n",
    "        sort_by=sort_by,\n",
    "        page_size=page_size\n",
    "    )\n",
    "\n",
    "    # Extract articles\n",
    "    articles = all_articles.get('articles', [])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if articles:\n",
    "        df = pd.DataFrame(articles)\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no articles are found\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now()\n",
    "# Get the time 10 days ago\n",
    "time_10_days_ago = current_time - timedelta(days=10)\n",
    "api_key = 'c0e23a8956cf4b54af382abd932f88ff'\n",
    "q = \"Microsoft News June 2024\"\n",
    "df = fetch_news(q, time_10_days_ago, current_time, api_key=api_key)\n",
    "\n",
    "df_news = df.drop(\"source\", axis=1)\n",
    "\n",
    "def preprocess_news_data(df):\n",
    "    # Convert publishedAt to datetime\n",
    "    df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n",
    "    df = df[~df['author'].isna()]\n",
    "    df = df[['author', 'title']]\n",
    "    return df\n",
    "\n",
    "preprocessed_news_df = preprocess_news_data(df_news)\n",
    "preprocessed_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a financial analyst tasked with providing insights into recent news articles related to the financial industry. Here are some recent news articles:\n",
      "\n",
      "   **News:** Google Sheets is getting faster and more effective, and I can't wait to ditch Excel for good\n",
      "\n",
      "   **News:** Hear from Microsoft Security experts at these top cybersecurity events in 2025\n",
      "\n",
      "   **News:** Every Game Delayed Right Now (2025 Edition)\n",
      "\n",
      "   **News:** Attackers used a public ASP.NET machine to conduct ViewState code injection attacks\n",
      "\n",
      "   **News:** Russia’s intelligence recruits Ukrainians for terror attacks via messaging apps\n",
      "\n",
      "   **News:** UK Gov demands backdoor to access Apple iCloud backups worldwide\n",
      "\n",
      "   **News:** U.S. CISA adds Trimble Cityworks flaw to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** Kimsuky APT group used custom RDP Wrapper version and forceCopy stealer\n",
      "\n",
      "   **News:** U.S. CISA adds Microsoft Outlook, Sophos XG Firewall, and other flaws to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** PlayStation Network outage has been going on for over 24 hours\n",
      "\n",
      "   **News:** SECURITY AFFAIRS MALWARE NEWSLETTER – ROUND 32\n",
      "\n",
      "   **News:** Security Affairs newsletter Round 510 by Pierluigi Paganini – INTERNATIONAL EDITION\n",
      "\n",
      "   **News:** Cisco addressed two critical flaws in its Identity Services Engine (ISE)\n",
      "\n",
      "   **News:** Author reveals more details on HarperCollins AI-deal royalties\n",
      "\n",
      "   **News:** U.S. CISA adds Linux kernel flaw to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** U.S. CISA adds Microsoft .NET Framework, Apache OFBiz, and Paessler PRTG Network Monitor flaws to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** Law enforcement seized the domains of HeartSender cybercrime marketplaces\n",
      "\n",
      "   **News:** Web Skimmer found on at least 17 websites, including Casio UK\n",
      "\n",
      "   **News:** Texas is the first state to ban DeepSeek on government devices\n",
      "\n",
      "   **News:** AMD fixed a flaw that allowed to load malicious microcode\n",
      "\n",
      "   **News:** Coyote Banking Trojan targets Brazilian users, stealing data from 70+ financial apps and websites\n",
      "\n",
      "   **News:** Google fixed actively exploited kernel zero-day flaw\n",
      "\n",
      "   **News:** Elon Musk ’s DOGE team granted ‘full access’ to sensitive Treasury systems. What are the risks?\n",
      "\n",
      "   **News:** WhatsApp disrupted a hacking campaign targeting journalists with Paragon spyware\n",
      "\n",
      "   **News:** Online food ordering and delivery platform GrubHub discloses a data breach\n",
      "\n",
      "   **News:** Netgear urges users to upgrade two flaws impacting WiFi router models\n",
      "\n",
      "   **News:** Security Affairs newsletter Round 509 by Pierluigi Paganini – INTERNATIONAL EDITION\n",
      "\n",
      "   **News:** International Civil Aviation Organization (ICAO) and ACAO Breached: Cyberespionage Groups Targeting Aviation Safety Specialists\n",
      "\n",
      "   **News:** High-School Band Contests Turn Marching Into a Sport—and an Art\n",
      "\n",
      "   **News:** Amazon to spend $100B in AWS AI infrastructure\n",
      "\n",
      "Please analyze these articles and provide insights into any potential impacts on the financial industry Sentiment on the provided company.\n"
     ]
    }
   ],
   "source": [
    "def build_prompt(news_df):\n",
    "    prompt = \"You are a financial analyst tasked with providing insights into recent news articles related to the financial industry. Here are some recent news articles:\\n\\n\"\n",
    "\n",
    "    for index, row in news_df.iterrows():\n",
    "        title = row['title']\n",
    "        prompt += f\"   **News:** {title}\\n\\n\"\n",
    "\n",
    "    prompt += \"Please analyze these articles and provide insights into any potential impacts on the financial industry Sentiment on the provided company.\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Build the prompt\n",
    "prompt = build_prompt(preprocessed_news_df)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RunnableSequence.invoke() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: RunnableSequence.invoke() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "chain.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
