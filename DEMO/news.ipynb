{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a financial analyst tasked with providing insights into recent news articles related to the financial industry. Here are some recent news articles:\n",
      "\n",
      "   **News:** Tesla's numbers disappoint again ... and the crowd goes wild again\n",
      "\n",
      "   **News:** Google Sheets is getting faster and more effective, and I can't wait to ditch Excel for good\n",
      "\n",
      "   **News:** LinkedIn’s video push appears to be working in 2025\n",
      "\n",
      "   **News:** Hear from Microsoft Security experts at these top cybersecurity events in 2025\n",
      "\n",
      "   **News:** Apple Loop: iPhone 17 Design, Tiktok Still Missing, iPhone SE Release Dates\n",
      "\n",
      "   **News:** Attackers used a public ASP.NET machine to conduct ViewState code injection attacks\n",
      "\n",
      "   **News:** U.S. CISA adds Microsoft Outlook, Sophos XG Firewall, and other flaws to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** Cisco addressed two critical flaws in its Identity Services Engine (ISE)\n",
      "\n",
      "   **News:** U.S. CISA adds Linux kernel flaw to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** U.S. CISA adds Microsoft .NET Framework, Apache OFBiz, and Paessler PRTG Network Monitor flaws to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** U.S. CISA adds Apple products’ flaw to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** Law enforcement seized the domains of HeartSender cybercrime marketplaces\n",
      "\n",
      "   **News:** Web Skimmer found on at least 17 websites, including Casio UK\n",
      "\n",
      "   **News:** Texas is the first state to ban DeepSeek on government devices\n",
      "\n",
      "   **News:** AMD fixed a flaw that allowed to load malicious microcode\n",
      "\n",
      "   **News:** DeepSeek database exposed highly sensitive information\n",
      "\n",
      "   **News:** Contec CMS8000 patient monitors contain a hidden backdoor\n",
      "\n",
      "   **News:** Coyote Banking Trojan targets Brazilian users, stealing data from 70+ financial apps and websites\n",
      "\n",
      "   **News:** TeamViewer fixed a vulnerability in Windows client and host applications\n",
      "\n",
      "   **News:** Italy’s data protection authority Garante blocked the DeepSeek AI platform\n",
      "\n",
      "   **News:** Google fixed actively exploited kernel zero-day flaw\n",
      "\n",
      "   **News:** Elon Musk ’s DOGE team granted ‘full access’ to sensitive Treasury systems. What are the risks?\n",
      "\n",
      "   **News:** WhatsApp disrupted a hacking campaign targeting journalists with Paragon spyware\n",
      "\n",
      "   **News:** Online food ordering and delivery platform GrubHub discloses a data breach\n",
      "\n",
      "   **News:** Netgear urges users to upgrade two flaws impacting WiFi router models\n",
      "\n",
      "   **News:** Community Health Center data breach impacted over 1 million patients\n",
      "\n",
      "   **News:** A ransomware attack forced New York Blood Center to reschedule appointments\n",
      "\n",
      "   **News:** PHP package Voyager flaws expose to one-click RCE exploits\n",
      "\n",
      "   **News:** Italy’s Data Protection Authority Garante requested information from Deepseek\n",
      "\n",
      "   **News:** Broadcom fixed information disclosure flaws in VMware Aria Operations\n",
      "\n",
      "Please analyze these articles and provide insights into any potential impacts on the financial industry Sentiment on the provided company.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_prompt(news_df):\n",
    "    prompt = \"You are a financial analyst tasked with providing insights into recent news articles related to the financial industry. Here are some recent news articles:\\n\\n\"\n",
    "\n",
    "    for index, row in news_df.iterrows():\n",
    "        title = row['title']\n",
    "        prompt += f\"   **News:** {title}\\n\\n\"\n",
    "\n",
    "    prompt += \"Please analyze these articles and provide insights into any potential impacts on the financial industry Sentiment on the provided company.\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Build the prompt\n",
    "prompt = build_prompt(preprocessed_news_df)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching news for: Apple Inc. (AAPL)\n",
      "⚠ Error fetching from Yahoo Finance: 'provider'\n",
      "\n",
      "✅ Successfully saved 60 news articles to AAPL_news.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "# Replace with your NewsAPI Key\n",
    "NEWSAPI_KEY = \"8afd555b84b84ee9a83a720ef0de8397\"\n",
    "\n",
    "# Initialize NewsAPI Client\n",
    "newsapi = NewsApiClient(api_key=NEWSAPI_KEY)\n",
    "\n",
    "# Ask user for stock ticker\n",
    "stock_symbol = input(\"Enter Stock Ticker (e.g., AAPL, TSLA, MSFT): \").upper()\n",
    "\n",
    "# Fetch company information from Yahoo Finance\n",
    "try:\n",
    "    stock_info = yf.Ticker(stock_symbol).info\n",
    "    company_name = stock_info.get(\"shortName\", stock_symbol)  # Use stock name or ticker\n",
    "    print(f\"\\nFetching news for: {company_name} ({stock_symbol})\")\n",
    "except Exception as e:\n",
    "    print(\"⚠ Unable to fetch stock details. Using ticker as the company name.\")\n",
    "    company_name = stock_symbol\n",
    "\n",
    "# Function to fetch news from NewsAPI\n",
    "def fetch_newsapi_news(company_name, max_pages=3):\n",
    "    articles_list = []\n",
    "    try:\n",
    "        for page in range(1, max_pages + 1):\n",
    "            news_data = newsapi.get_everything(\n",
    "                q=company_name,\n",
    "                language=\"en\",\n",
    "                sort_by=\"publishedAt\",\n",
    "                page_size=20,\n",
    "                page=page  # Fetch multiple pages\n",
    "            )\n",
    "            if \"articles\" in news_data:\n",
    "                for article in news_data[\"articles\"]:\n",
    "                    articles_list.append({\n",
    "                        \"source\": article[\"source\"][\"name\"],\n",
    "                        \"title\": article[\"title\"],\n",
    "                        \"description\": article[\"description\"],\n",
    "                        \"url\": article[\"url\"],\n",
    "                        \"published_at\": article[\"publishedAt\"],\n",
    "                        \"source_type\": \"NewsAPI\"\n",
    "                    })\n",
    "            if len(news_data[\"articles\"]) < 20:  # Stop if fewer than 20 articles on a page\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error fetching from NewsAPI: {e}\")\n",
    "\n",
    "    return articles_list\n",
    "\n",
    "\n",
    "# Function to fetch stock news from Yahoo Finance\n",
    "def fetch_yahoo_news(stock_symbol):\n",
    "    articles_list = []\n",
    "    try:\n",
    "        stock = yf.Ticker(stock_symbol)\n",
    "        news_data = stock.news  # Fetch news\n",
    "\n",
    "        for article in news_data:\n",
    "            articles_list.append({\n",
    "                \"source\": article[\"provider\"],\n",
    "                \"title\": article[\"title\"],\n",
    "                \"description\": article.get(\"summary\", \"\"),\n",
    "                \"url\": article[\"link\"],\n",
    "                \"published_at\": pd.to_datetime(article[\"providerPublishTime\"], unit=\"s\"),\n",
    "                \"source_type\": \"Yahoo Finance\"\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error fetching from Yahoo Finance: {e}\")\n",
    "\n",
    "    return articles_list\n",
    "\n",
    "# Fetch news from both sources\n",
    "newsapi_news = fetch_newsapi_news(company_name)\n",
    "yahoo_news = fetch_yahoo_news(stock_symbol)\n",
    "\n",
    "# Combine all news articles\n",
    "all_news = newsapi_news + yahoo_news\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_news)\n",
    "\n",
    "# Save to CSV file\n",
    "csv_filename = f\"{stock_symbol}_news.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"\\n✅ Successfully saved {len(df)} news articles to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Ensure DataFrame exists and is not empty\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠ No news data available.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     exit()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure DataFrame exists and is not empty\n",
    "if df.empty:\n",
    "    print(\"⚠ No news data available.\")\n",
    "    exit()\n",
    "\n",
    "# Convert news articles into structured paragraphs\n",
    "df[\"formatted_news\"] = df.apply(\n",
    "    lambda row: (\n",
    "        f\"News {row.name + 1}: {row['title']}.\\n\"\n",
    "        f\"Source: {row['source']} | Published: {row['published_at']}.\\n\"\n",
    "        f\"Description: {row['description']}\\n\"\n",
    "        f\"Read more: {row['url']}\\n\"\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Combine all news into a single text block\n",
    "final_text = \"\\n\\n\".join(df[\"formatted_news\"])\n",
    "\n",
    "# Save formatted text to a file\n",
    "text_filename = \"formatted_stock_news.txt\"\n",
    "with open(text_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(final_text)\n",
    "\n",
    "print(f\"\\n✅ Successfully saved structured news to {text_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks stored in FAISS successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "\n",
    "import os \n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# Set Google API Key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDxhPMlJLbGHMvBzXbgV8ldG9-JlReq1Hg\"\n",
    "\n",
    "# Load Google Generative AI embeddings model\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    model=\"models/embedding-001\"  # Correct model name\n",
    ")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",temperature=0.3, max_tokens=500)\n",
    "# llm= OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "\n",
    "# Load text file\n",
    "file_path = \"formatted_stock_news.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()  # Read entire content of the file\n",
    "\n",
    "# Convert each text file into a Document object\n",
    "documents = [Document(page_content=text_data)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "chunked_docs = text_splitter.split_documents(documents)\n",
    "db = FAISS.from_documents(chunked_docs, embedding_model)\n",
    "\n",
    "# Save FAISS index for future use\n",
    "db.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"Chunks stored in FAISS successfully!\")\n",
    "\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "system_prompt = (\"\"\"\"\n",
    "                 Act As an Financial News Summarizer Analyst . I have Provided you some key Financial News about stocks news  give answer according to it \n",
    "\n",
    "\"\"\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response appended to company_overview.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\" Summarize All News With Analysis Try to cover All the News dont miss out Any News Specially News With Numbers indicators  . it Should be detailed    \"\"\"\n",
    "\n",
    "response = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "# Extract the answer text\n",
    "response_text = response[\"answer\"]\n",
    "\n",
    "# Save the response to a text file (Append mode)\n",
    "with open(\"news_summarizer.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "\n",
    "    file.write(response_text + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"Response appended to company_overview.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model\n",
    "chain.invoke({\"question\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technique 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mike Moore</td>\n",
       "      <td>Google Sheets is getting faster and more effec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Benjamin Lim</td>\n",
       "      <td>Hear from Microsoft Security experts at these ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gabe Gurwin</td>\n",
       "      <td>Every Game Delayed Right Now (2025 Edition)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pierluigi Paganini</td>\n",
       "      <td>Attackers used a public ASP.NET machine to con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pierluigi Paganini</td>\n",
       "      <td>Russia’s intelligence recruits Ukrainians for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                              title\n",
       "0          Mike Moore  Google Sheets is getting faster and more effec...\n",
       "1        Benjamin Lim  Hear from Microsoft Security experts at these ...\n",
       "2         Gabe Gurwin        Every Game Delayed Right Now (2025 Edition)\n",
       "3  Pierluigi Paganini  Attackers used a public ASP.NET machine to con...\n",
       "4  Pierluigi Paganini  Russia’s intelligence recruits Ukrainians for ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from newsapi import NewsApiClient\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_news(query, from_date, to_date, language='en', sort_by='relevancy', page_size=30, api_key='8afd555b84b84ee9a83a720ef0de8397'):\n",
    "    # Initialize the NewsAPI client\n",
    "    newsapi = NewsApiClient(api_key=api_key)\n",
    "    query = query.replace(' ','&')\n",
    "    # Fetch all articles matching the query\n",
    "    all_articles = newsapi.get_everything(\n",
    "        q=query,\n",
    "        from_param=from_date,\n",
    "        to=to_date,\n",
    "        language=language,\n",
    "        sort_by=sort_by,\n",
    "        page_size=page_size\n",
    "    )\n",
    "\n",
    "    # Extract articles\n",
    "    articles = all_articles.get('articles', [])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if articles:\n",
    "        df = pd.DataFrame(articles)\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no articles are found\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now()\n",
    "# Get the time 10 days ago\n",
    "time_10_days_ago = current_time - timedelta(days=10)\n",
    "api_key = 'c0e23a8956cf4b54af382abd932f88ff'\n",
    "q = \"Microsoft News June 2024\"\n",
    "df = fetch_news(q, time_10_days_ago, current_time, api_key=api_key)\n",
    "\n",
    "df_news = df.drop(\"source\", axis=1)\n",
    "\n",
    "def preprocess_news_data(df):\n",
    "    # Convert publishedAt to datetime\n",
    "    df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n",
    "    df = df[~df['author'].isna()]\n",
    "    df = df[['author', 'title']]\n",
    "    return df\n",
    "\n",
    "preprocessed_news_df = preprocess_news_data(df_news)\n",
    "preprocessed_news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a financial analyst tasked with providing insights into recent news articles related to the financial industry. Here are some recent news articles:\n",
      "\n",
      "   **News:** Google Sheets is getting faster and more effective, and I can't wait to ditch Excel for good\n",
      "\n",
      "   **News:** Hear from Microsoft Security experts at these top cybersecurity events in 2025\n",
      "\n",
      "   **News:** Every Game Delayed Right Now (2025 Edition)\n",
      "\n",
      "   **News:** Attackers used a public ASP.NET machine to conduct ViewState code injection attacks\n",
      "\n",
      "   **News:** Russia’s intelligence recruits Ukrainians for terror attacks via messaging apps\n",
      "\n",
      "   **News:** UK Gov demands backdoor to access Apple iCloud backups worldwide\n",
      "\n",
      "   **News:** U.S. CISA adds Trimble Cityworks flaw to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** Kimsuky APT group used custom RDP Wrapper version and forceCopy stealer\n",
      "\n",
      "   **News:** U.S. CISA adds Microsoft Outlook, Sophos XG Firewall, and other flaws to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** PlayStation Network outage has been going on for over 24 hours\n",
      "\n",
      "   **News:** SECURITY AFFAIRS MALWARE NEWSLETTER – ROUND 32\n",
      "\n",
      "   **News:** Security Affairs newsletter Round 510 by Pierluigi Paganini – INTERNATIONAL EDITION\n",
      "\n",
      "   **News:** Cisco addressed two critical flaws in its Identity Services Engine (ISE)\n",
      "\n",
      "   **News:** Author reveals more details on HarperCollins AI-deal royalties\n",
      "\n",
      "   **News:** U.S. CISA adds Linux kernel flaw to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** U.S. CISA adds Microsoft .NET Framework, Apache OFBiz, and Paessler PRTG Network Monitor flaws to its Known Exploited Vulnerabilities catalog\n",
      "\n",
      "   **News:** Law enforcement seized the domains of HeartSender cybercrime marketplaces\n",
      "\n",
      "   **News:** Web Skimmer found on at least 17 websites, including Casio UK\n",
      "\n",
      "   **News:** Texas is the first state to ban DeepSeek on government devices\n",
      "\n",
      "   **News:** AMD fixed a flaw that allowed to load malicious microcode\n",
      "\n",
      "   **News:** Coyote Banking Trojan targets Brazilian users, stealing data from 70+ financial apps and websites\n",
      "\n",
      "   **News:** Google fixed actively exploited kernel zero-day flaw\n",
      "\n",
      "   **News:** Elon Musk ’s DOGE team granted ‘full access’ to sensitive Treasury systems. What are the risks?\n",
      "\n",
      "   **News:** WhatsApp disrupted a hacking campaign targeting journalists with Paragon spyware\n",
      "\n",
      "   **News:** Online food ordering and delivery platform GrubHub discloses a data breach\n",
      "\n",
      "   **News:** Netgear urges users to upgrade two flaws impacting WiFi router models\n",
      "\n",
      "   **News:** Security Affairs newsletter Round 509 by Pierluigi Paganini – INTERNATIONAL EDITION\n",
      "\n",
      "   **News:** International Civil Aviation Organization (ICAO) and ACAO Breached: Cyberespionage Groups Targeting Aviation Safety Specialists\n",
      "\n",
      "   **News:** High-School Band Contests Turn Marching Into a Sport—and an Art\n",
      "\n",
      "   **News:** Amazon to spend $100B in AWS AI infrastructure\n",
      "\n",
      "Please analyze these articles and provide insights into any potential impacts on the financial industry Sentiment on the provided company.\n"
     ]
    }
   ],
   "source": [
    "def build_prompt(news_df):\n",
    "    prompt = \"You are a financial analyst tasked with providing insights into recent news articles related to the financial industry. Here are some recent news articles:\\n\\n\"\n",
    "\n",
    "    for index, row in news_df.iterrows():\n",
    "        title = row['title']\n",
    "        prompt += f\"   **News:** {title}\\n\\n\"\n",
    "\n",
    "    prompt += \"Please analyze these articles and provide insights into any potential impacts on the financial industry Sentiment on the provided company.\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# Build the prompt\n",
    "prompt = build_prompt(preprocessed_news_df)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I\\'m trying to figure out how to solve this problem where I need to write a Python function that can process some text and then answer questions based on it. The user provided an example where they input \"What is the capital of France?\" and get an answer saying \"Let\\'s think step by step.\" Hmm, that seems a bit confusing because the response isn\\'t actually giving the correct information. Maybe there was a mistake in how the function processes the input.\\n\\nI remember from my previous experience that sometimes when dealing with question-answering systems, especially if they\\'re based on pre-trained models or APIs, you need to make sure that you\\'re correctly formatting your inputs and handling the responses properly. Let me try to break down what\\'s happening here.\\n\\nFirst, in the example provided, the user input is \"What is the capital of France?\" which should ideally trigger a response from the function providing \"Paris.\" Instead, it just says \"Let\\'s think step by step.\" That makes me think that either the function isn\\'t properly understanding the question or there\\'s an issue with how the responses are being generated.\\n\\nI wonder if the function is correctly parsing the input variables. The user mentioned that the input_variables list includes \\'question\\', which should be passed to the prompt template. So in the example, when you replace {question} with \"What is the capital of France?\", the prompt becomes \"Question: What is the capital of France?\\\\n\\\\n\\\\nAnswer: Let\\'s think step by step.\" That seems correct on the surface, but perhaps the system isn\\'t processing this correctly.\\n\\nMaybe there\\'s a problem with how the answers are being fetched. If the function uses an external API or model for generating responses, there could be delays or issues in communication. For example, if it\\'s making a call to an external service, there might be timeouts or errors that aren\\'t being handled properly, causing the response to be generic like \"Let\\'s think step by step.\"\\n\\nAnother possibility is that the function isn\\'t correctly processing the input types or partial variables. In the user\\'s prompt template, they have input_types={} and partial_variables={}, so maybe some necessary information is missing. If the function expects certain data formats, but it\\'s not receiving them, it might default to a generic response.\\n\\nI should also consider how the function handles cases where it doesn\\'t have enough information or when the question isn\\'t framed correctly. In this case, since the example question is straightforward, I\\'m not sure why the answer isn\\'t being generated properly.\\n\\nPerhaps there\\'s an issue with how the prompt template is structured. Maybe adding more specific instructions or parameters to the prompt could help. For instance, if the model requires a certain format for questions, like using slots or entities, the function might need to extract those from the input.\\n\\nI\\'m also thinking about whether the function has access to the right dataset. If it\\'s supposed to answer based on a specific knowledge base, maybe there\\'s an issue with connectivity or data retrieval. In this case, since the user didn\\'t mention any external data sources, perhaps it\\'s more of an internal processing issue.\\n\\nAnother angle is that the function might not be correctly handling asynchronous responses. If it\\'s making multiple API calls or waiting for results, delays could lead to the system appearing unresponsive or giving placeholder messages like \"Let\\'s think step by step.\"\\n\\nI should also check if there are any error handling mechanisms in place. Does the function catch exceptions and handle them gracefully? If not, unexpected issues might cause it to fail silently or provide incorrect responses.\\n\\nAdditionally, maybe the prompt template isn\\'t providing enough context for the model to generate a proper answer. Adding more information or instructions could guide the model better. For example, specifying that the answer should be concise or include specific details based on the question.\\n\\nI wonder if the function is using natural language processing tools likespaCy or something else to parse and understand the question. If there\\'s an error during parsing, it might not generate a meaningful response. Checking the logs or debugging outputs could provide more insight into where things are going wrong.\\n\\nAlso, considering that the response is \"Let\\'s think step by step,\" perhaps the function is using a generic default response when it can\\'t process the request or find the information. Investigating why this response is being triggered could help identify the root cause.\\n\\nMaybe there\\'s an issue with how the variables are being passed into the prompt template. If \\'question\\' isn\\'t correctly inserted, the model might receive a malformed prompt, leading to unexpected responses. Ensuring that variable substitution works correctly is crucial.\\n\\nI should also think about caching mechanisms. If the function caches answers and there\\'s a delay in updating the cache, it might serve outdated information or default responses. Checking if the cache is functioning properly is another step to consider.\\n\\nLastly, reviewing the overall architecture of the function could reveal potential bottlenecks or areas where improvements are needed. Understanding how data flows through the system and ensuring each component is working as expected is important for troubleshooting.\\n\\nIn summary, possible reasons for the issue include problems with input parsing, incorrect handling of external services, missing information in prompts, error handling issues, or architectural problems within the function itself. To resolve this, I would need to systematically check each component of the function, test different inputs, and review logs or outputs for any errors or warning messages.\\n</think>\\n\\nTo address the issue where the function is not providing the expected response (\"Paris\" instead of a generic message), here\\'s a structured approach:\\n\\n1. **Verify Input Parsing**: Ensure that the input variable \\'question\\' is correctly inserted into the prompt template. In this case, \"What is the capital of France?\" should be properly substituted.\\n\\n2. **Check External Services and APIs**: Investigate if the function relies on external APIs or models for generating responses. If there are connectivity issues or delays, it might result in a default response. Ensure these services are functioning correctly.\\n\\n3. **Examine Prompt Structure**: Review the prompt template to ensure it provides sufficient context and instructions. Adding more specific instructions could help guide the model better, especially if it requires a particular format for questions.\\n\\n4. **Error Handling Mechanisms**: Check if the function includes robust error handling to catch exceptions and provide meaningful responses instead of default messages. Implement logging to track any issues during processing.\\n\\n5. **Data Retrieval and Caching**: Ensure that external data sources or caches are functioning properly. If information is outdated, clear the cache and refresh it to get up-to-date results.\\n\\n6. **Natural Language Processing Tools**: Verify that tools like spaCy or other NLP tools are correctly parsing and understanding the input. Check for any errors during parsing that might lead to incorrect or generic responses.\\n\\n7. **Review Architecture and Logs**: Analyze the overall function architecture for potential bottlenecks or areas of concern. Review logs or output messages for any warnings or errors that could indicate the root cause.\\n\\nBy systematically addressing each component, you can identify why the function isn\\'t providing the expected response and implement fixes accordingly.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP_Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
